# RAG Using LangChain with Google Gemini

## Summary

This project demonstrates the implementation of **Retrieval-Augmented Generation (RAG)** using the **LangChain** library and **Google Gemini Pro model**. The goal was to build a text-to-text application that utilizes **Generative AI** for producing contextual responses based on user prompts.

## Key Features

- **Google Gemini-2.0-flash-exp**:  
  Integrated and experimented with various configuration parameters such as **temperature**, **top_k**, and **top_p** to control the model's response behavior.

- **Retrieval-Augmented Generation (RAG)**:  
  Utilized **RAG techniques** to enhance text generation by fetching relevant context from large documents or datasets.

- **LangChain Integration**:  
  Leveraged **LangChain** to manage the flow of data and ensure high-quality, contextually aware text responses.

- **Customizable Configuration**:  
  Experimented with various model settings (e.g., **temperature**, **top_k**, **max tokens**) to fine-tune the modelâ€™s output and response quality.

- **Save Chat History**:  
  Implemented functionality to **save chat history**, enabling analysis of the model's responses over time for different user inputs.

## Skills & Technologies

- **Machine Learning**
- **Natural Language Processing (NLP)**
- **Generative AI**
- **LangChain**
- **Python**


This project provides a strong foundation for building more advanced **LLM** (Large Language Model) applications. It can be extended to handle more complex use cases such as **text generation**, **summarization**, and even **image generation** tasks. 

Feel free to explore, experiment, and customize it for your needs!
